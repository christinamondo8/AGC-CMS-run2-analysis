{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc37683",
   "metadata": {},
   "source": [
    "# CMS Run 2 $t\\bar{t}$: from data delivery to statistical inference\n",
    "\n",
    "We are using [2015 CMS Open Data](https://cms.cern/news/first-cms-open-data-lhc-run-2-released) in this demonstration to showcase an analysis pipeline.\n",
    "It features data delivery and processing, histogram construction and visualization, as well as statistical inference.\n",
    "\n",
    "This notebook was developed in the context of the [IRIS-HEP AGC tools 2022 workshop](https://indico.cern.ch/e/agc-tools-2).\n",
    "This work was supported by the U.S. National Science Foundation (NSF) Cooperative Agreement OAC-1836650 (IRIS-HEP).\n",
    "\n",
    "This is a **technical demonstration**.\n",
    "We are including the relevant workflow aspects that physicists need in their work, but we are not focusing on making every piece of the demonstration physically meaningful.\n",
    "This concerns in particular systematic uncertainties: we capture the workflow, but the actual implementations are more complex in practice.\n",
    "If you are interested in the physics side of analyzing top pair production, check out the latest results from [ATLAS](https://twiki.cern.ch/twiki/bin/view/AtlasPublic/TopPublicResults) and [CMS](https://cms-results.web.cern.ch/cms-results/public-results/preliminary-results/)!\n",
    "If you would like to see more technical demonstrations, also check out an [ATLAS Open Data example](https://indico.cern.ch/event/1076231/contributions/4560405/) demonstrated previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72d323",
   "metadata": {},
   "source": [
    "### Imports: setting up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b3b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "import cloudpickle\n",
    "import correctionlib\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "import copy\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "\n",
    "import utils  # contains code for bookkeeping and cosmetics, as well as some boilerplate\n",
    "\n",
    "logging.getLogger(\"cabinetry\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd573bb1",
   "metadata": {},
   "source": [
    "### Configuration: number of files and data delivery path\n",
    "\n",
    "The number of files per sample set here determines the size of the dataset we are processing. There are 9 samples being used here, all part of the 2015 CMS Open Data release.\n",
    "\n",
    "These samples were originally published in miniAOD format, but for the purposes of this demonstration were pre-converted into nanoAOD format. More details about the inputs can be found [here](https://github.com/iris-hep/analysis-grand-challenge/tree/main/datasets/cms-open-data-2015).\n",
    "\n",
    "The table below summarizes the amount of data processed depending on the `N_FILES_MAX_PER_SAMPLE` setting.\n",
    "\n",
    "| setting | number of files | total size | number of events |\n",
    "| --- | --- | --- | --- |\n",
    "| `1` | 9 | 22.9 GB | 10455719 |\n",
    "| `2` | 18 | 42.8 GB | 19497435 |\n",
    "| `5` | 43 | 105 GB | 47996231 |\n",
    "| `10` | 79 | 200 GB | 90546458 |\n",
    "| `20` | 140 | 359 GB | 163123242 |\n",
    "| `50` | 255 | 631 GB | 297247463 |\n",
    "| `100` | 395 | 960 GB | 470397795 |\n",
    "| `200` | 595 | 1.40 TB | 705273291 |\n",
    "| `-1` | 787 | 1.78 TB | 940160174 |\n",
    "\n",
    "The input files are all in the 1â€“3 GB range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43e9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "# input files per process, set to e.g. 10 (smaller number = faster)\n",
    "N_FILES_MAX_PER_SAMPLE = 5\n",
    "\n",
    "# enable Dask\n",
    "USE_DASK = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d6520",
   "metadata": {},
   "source": [
    "### Defining our `coffea` Processor\n",
    "\n",
    "The processor includes a lot of the physics analysis details:\n",
    "- event filtering and the calculation of observables,\n",
    "- event weighting,\n",
    "- calculating systematic uncertainties at the event and object level,\n",
    "- filling all the information into histograms that get aggregated and ultimately returned to us by `coffea`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee2b3a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TtbarAnalysis(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "\n",
    "        # initialize dictionary of hists for signal and control region\n",
    "        self.hist_dict = {}\n",
    "        for region in [\"4j1b\", \"4j2b\"]:\n",
    "            self.hist_dict[region] = (\n",
    "                hist.Hist.new.Reg(utils.config[\"global\"][\"NUM_BINS\"], \n",
    "                                  utils.config[\"global\"][\"BIN_LOW\"], \n",
    "                                  utils.config[\"global\"][\"BIN_HIGH\"], \n",
    "                                  name=\"observable\", \n",
    "                                  label=\"observable [GeV]\")\n",
    "                .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "                .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "                .Weight()\n",
    "            )\n",
    "        \n",
    "        self.cset = correctionlib.CorrectionSet.from_file(\"corrections.json\")\n",
    "\n",
    "    def only_do_IO(self, events):\n",
    "        for branch in utils.config[\"benchmarking\"][\"IO_BRANCHES\"][\n",
    "            utils.config[\"benchmarking\"][\"IO_FILE_PERCENT\"]\n",
    "        ]:\n",
    "            if \"_\" in branch:\n",
    "                split = branch.split(\"_\")\n",
    "                object_type = split[0]\n",
    "                property_name = \"_\".join(split[1:])\n",
    "                ak.materialized(events[object_type][property_name])\n",
    "            else:\n",
    "                ak.materialized(events[branch])\n",
    "        return {\"hist\": {}}\n",
    "\n",
    "    def process(self, events):\n",
    "        if utils.config[\"benchmarking\"][\"DISABLE_PROCESSING\"]:\n",
    "            # IO testing with no subsequent processing\n",
    "            return self.only_do_IO(events)\n",
    "\n",
    "        # create copies of histogram objects\n",
    "        hist_dict = copy.deepcopy(self.hist_dict)\n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "        is_data = events.metadata[\"is_data\"]\n",
    "\n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        \n",
    "        if is_data:\n",
    "            process = \"data\"\n",
    "            xsec_weight = 1\n",
    "        else:\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "\n",
    "        #### systematics\n",
    "        # jet energy scale / resolution systematics\n",
    "        # need to adjust schema to instead use coffea add_systematic feature, especially for ServiceX\n",
    "        # cannot attach pT variations to events.jet, so attach to events directly\n",
    "        # and subsequently scale pT by these scale factors\n",
    "        events[\"pt_scale_up\"] = 1.03\n",
    "        events[\"pt_res_up\"] = utils.systematics.jet_pt_resolution(events.Jet.pt)\n",
    "\n",
    "        syst_variations = [\"nominal\"]\n",
    "        jet_kinematic_systs = [\"pt_scale_up\", \"pt_res_up\"]\n",
    "        event_systs = [f\"btag_var_{i}\" for i in range(4)]\n",
    "        if process == \"wjets\":\n",
    "            event_systs.append(\"scale_var\")\n",
    "\n",
    "        # Only do systematics for nominal samples, e.g. ttbar__nominal\n",
    "        if variation == \"nominal\":\n",
    "            syst_variations.extend(jet_kinematic_systs)\n",
    "            syst_variations.extend(event_systs)\n",
    "\n",
    "        # for pt_var in pt_variations:\n",
    "        for syst_var in syst_variations:\n",
    "            if is_data and syst_var != \"nominal\":\n",
    "                continue\n",
    "            ### event selection\n",
    "            # very very loosely based on https://arxiv.org/abs/2006.13076\n",
    "\n",
    "            # Note: This creates new objects, distinct from those in the 'events' object\n",
    "            elecs = events.Electron\n",
    "            muons = events.Muon\n",
    "            jets = events.Jet\n",
    "            if syst_var in jet_kinematic_systs:\n",
    "                # Replace jet.pt with the adjusted values\n",
    "                jets[\"pt\"] = jets.pt * events[syst_var]\n",
    "\n",
    "            electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "            muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                         (muons.pfRelIso04_all < 0.15))\n",
    "            jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "            # Only keep objects that pass our requirements\n",
    "            elecs = elecs[electron_reqs]\n",
    "            muons = muons[muon_reqs]\n",
    "            jets = jets[jet_reqs]\n",
    "\n",
    "            B_TAG_THRESHOLD = 0.5\n",
    "\n",
    "            ######### Store boolean masks with PackedSelection ##########\n",
    "            selections = PackedSelection(dtype='uint64')\n",
    "            # Basic selection criteria\n",
    "            selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "            selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "            selections.add(\"exactly_1b\", ak.sum(jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) == 1)\n",
    "            selections.add(\"atleast_2b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "            # Complex selection criteria\n",
    "            selections.add(\"4j1b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"exactly_1b\"))\n",
    "            selections.add(\"4j2b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_2b\"))\n",
    "\n",
    "            for region in [\"4j1b\", \"4j2b\"]:\n",
    "                region_selection = selections.all(region)\n",
    "                region_jets = jets[region_selection]\n",
    "                region_elecs = elecs[region_selection]\n",
    "                region_muons = muons[region_selection]\n",
    "                region_weights = np.ones(len(region_jets)) * xsec_weight\n",
    "\n",
    "                if region == \"4j1b\":\n",
    "                    observable = ak.sum(region_jets.pt, axis=-1)\n",
    "\n",
    "                elif region == \"4j2b\":\n",
    "\n",
    "                    # reconstruct hadronic top as bjj system with largest pT\n",
    "                    trijet = ak.combinations(region_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "                    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "                    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "                    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "                    # pick trijet candidate with largest pT and calculate mass of system\n",
    "                    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "                    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "                    if sum(region_selection)==0:\n",
    "                        continue\n",
    "\n",
    "                syst_var_name = f\"{syst_var}\"\n",
    "                # Break up the filling into event weight systematics and object variation systematics\n",
    "                if syst_var in event_systs:\n",
    "                    for i_dir, direction in enumerate([\"up\", \"down\"]):\n",
    "                        # Should be an event weight systematic with an up/down variation\n",
    "                        if syst_var.startswith(\"btag_var\"):\n",
    "                            i_jet = int(syst_var.rsplit(\"_\",1)[-1])   # Kind of fragile\n",
    "                            wgt_variation = self.cset[\"event_systematics\"].evaluate(\"btag_var\", direction, region_jets.pt[:,i_jet])\n",
    "                        elif syst_var == \"scale_var\":\n",
    "                            # The pt array is only used to make sure the output array has the correct shape\n",
    "                            wgt_variation = self.cset[\"event_systematics\"].evaluate(\"scale_var\", direction, region_jets.pt[:,0])\n",
    "                        syst_var_name = f\"{syst_var}_{direction}\"\n",
    "                        hist_dict[region].fill(\n",
    "                            observable=observable, process=process,\n",
    "                            variation=syst_var_name, weight=region_weights * wgt_variation\n",
    "                        )\n",
    "                else:\n",
    "                    # Should either be 'nominal' or an object variation systematic\n",
    "                    if variation != \"nominal\":\n",
    "                        # This is a 2-point systematic, e.g. ttbar__scaledown, ttbar__ME_var, etc.\n",
    "                        syst_var_name = variation\n",
    "                    hist_dict[region].fill(\n",
    "                        observable=observable, process=process,\n",
    "                        variation=syst_var_name, weight=region_weights\n",
    "                    )\n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)}, \"hist_dict\": hist_dict}\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd4c9e",
   "metadata": {},
   "source": [
    "### \"Fileset\" construction and metadata\n",
    "\n",
    "Here, we gather all the required information about the files we want to process: paths to the files and asociated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf166ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes in fileset: ['SingleMuon_2018A__nominal', 'SingleMuon_2018B__nominal']\n"
     ]
    }
   ],
   "source": [
    "fileset = utils.file_input.construct_fileset(\n",
    "    \"Run2_data.json\",\n",
    "    N_FILES_MAX_PER_SAMPLE, \n",
    "    use_xcache=False, \n",
    "    af_name=utils.config[\"benchmarking\"][\"AF_NAME\"],\n",
    ")  # local files on /data for ssl-dev\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "#print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "#print(f\"  'metadata': {fileset['ttbar__nominal']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910d3d5",
   "metadata": {},
   "source": [
    "### Execute the data delivery pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a032a148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/distributed/client.py:1288: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+----------------+----------------+----------------+\n",
      "| Package | client         | scheduler      | workers        |\n",
      "+---------+----------------+----------------+----------------+\n",
      "| python  | 3.8.16.final.0 | 3.8.16.final.0 | 3.8.15.final.0 |\n",
      "+---------+----------------+----------------+----------------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 13.6s5s\r"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed processing file: WorkItem(dataset='SingleMuon_2018B__nominal', filename='root://xcache//store/data/Run2018B/SingleMuon/NANOAOD/UL2018_MiniAODv2_NanoAODv9-v2/250000/2A561E4B-70D2-F343-A895-58E5B387E556.root', treename='Events', entrystart=0, entrystop=190548, fileuuid=b'\\r\\xcf\\xccn\\x11H\\x11\\xec\\x96B\\x8c\\xcc\\xe1\\x83\\xbe\\xef', usermeta={'xsec': 1, 'variation': 'nominal', 'nevts': -5, 'process': 'SingleMuon_2018B'})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1654\u001b[0m, in \u001b[0;36m_work_function\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1654\u001b[0m     out \u001b[38;5;241m=\u001b[39m processor_instance\u001b[38;5;241m.\u001b[39mprocess(events)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m events[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt_scale_up\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.03\u001b[39m\n\u001b[0;32m---> 58\u001b[0m events[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt_res_up\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39msystematics\u001b[38;5;241m.\u001b[39mjet_pt_resolution(events\u001b[38;5;241m.\u001b[39mJet\u001b[38;5;241m.\u001b[39mpt)\n\u001b[1;32m     60\u001b[0m syst_variations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnominal\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'systematics'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# processing\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m all_histograms, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfileset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtreename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTtbarAnalysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m exec_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mexecution took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexec_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1700\u001b[0m, in \u001b[0;36mRunner.__call__\u001b[0;34m(self, fileset, treename, processor_instance)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1681\u001b[0m     fileset: Dict,\n\u001b[1;32m   1682\u001b[0m     treename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   1683\u001b[0m     processor_instance: ProcessorABC,\n\u001b[1;32m   1684\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Accumulatable:\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the processor_instance on a given fileset\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \n\u001b[1;32m   1687\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;124;03m            An instance of a class deriving from ProcessorABC\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1700\u001b[0m     wrapped_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dataframes:\n\u001b[1;32m   1702\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_out  \u001b[38;5;66;03m# not wrapped anymore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1848\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self, fileset, processor_instance, treename)\u001b[0m\n\u001b[1;32m   1843\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomatic_retries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipbadfiles, closure\n\u001b[1;32m   1845\u001b[0m )\n\u001b[1;32m   1847\u001b[0m executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexe_args)\n\u001b[0;32m-> 1848\u001b[0m wrapped_out, e \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrapped_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1851\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo chunks returned results, verify ``processor`` instance structure.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;124m        if you used skipbadfiles=True, it is possible all your files are bad.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1853\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:974\u001b[0m, in \u001b[0;36mDaskExecutor.__call__\u001b[0;34m(self, items, function, accumulator)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;66;03m# FIXME: fancy widget doesn't appear, have to live with boring pbar\u001b[39;00m\n\u001b[1;32m    968\u001b[0m         progress(work, multi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, notebook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    970\u001b[0m         accumulate(\n\u001b[1;32m    971\u001b[0m             [\n\u001b[1;32m    972\u001b[0m                 work\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    973\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 974\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m _decompress(\u001b[43mwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    975\u001b[0m             ],\n\u001b[1;32m    976\u001b[0m             accumulator,\n\u001b[1;32m    977\u001b[0m         ),\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m KilledWorker \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    981\u001b[0m     baditem \u001b[38;5;241m=\u001b[39m key_to_item[ex\u001b[38;5;241m.\u001b[39mtask]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/distributed/client.py:282\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    281\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:221\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 221\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compress(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevel)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1367\u001b[0m, in \u001b[0;36mautomatic_retries\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1363\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m skipbadfiles\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuth failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chain)\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m retries \u001b[38;5;241m==\u001b[39m retry_count\n\u001b[1;32m   1366\u001b[0m     ):\n\u001b[0;32m-> 1367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1368\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (retry_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1369\u001b[0m retry_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1336\u001b[0m, in \u001b[0;36mautomatic_retries\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m retries:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1336\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1337\u001b[0m     \u001b[38;5;66;03m# catch xrootd errors and optionally skip\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;66;03m# or retry to read the file\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py:1656\u001b[0m, in \u001b[0;36m_work_function\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1654\u001b[0m     out \u001b[38;5;241m=\u001b[39m processor_instance\u001b[38;5;241m.\u001b[39mprocess(events)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1656\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed processing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput of process() should not be None. Make sure your processor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms process() function returns an accumulator.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1660\u001b[0m     )\n",
      "\u001b[0;31mException\u001b[0m: Failed processing file: WorkItem(dataset='SingleMuon_2018B__nominal', filename='root://xcache//store/data/Run2018B/SingleMuon/NANOAOD/UL2018_MiniAODv2_NanoAODv9-v2/250000/2A561E4B-70D2-F343-A895-58E5B387E556.root', treename='Events', entrystart=0, entrystop=190548, fileuuid=b'\\r\\xcf\\xccn\\x11H\\x11\\xec\\x96B\\x8c\\xcc\\xe1\\x83\\xbe\\xef', usermeta={'xsec': 1, 'variation': 'nominal', 'nevts': -5, 'process': 'SingleMuon_2018B'})"
     ]
    }
   ],
   "source": [
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "if USE_DASK:\n",
    "    cloudpickle.register_pickle_by_value(utils) # serialize methods and objects in utils so that they can be accessed within the coffea processor\n",
    "    executor = processor.DaskExecutor(client=utils.clients.get_client(af=utils.config[\"global\"][\"AF\"]))\n",
    "else:\n",
    "    executor = processor.FuturesExecutor(workers=utils.config[\"benchmarking\"][\"NUM_CORES\"])\n",
    "\n",
    "run = processor.Runner(\n",
    "    executor=executor, \n",
    "    schema=NanoAODSchema, \n",
    "    savemetrics=True, \n",
    "    metadata_cache={}, \n",
    "    chunksize=utils.config[\"benchmarking\"][\"CHUNKSIZE\"])\n",
    "\n",
    "treename = \"Events\"\n",
    "filemeta = run.preprocess(fileset, treename=treename)  # pre-processing\n",
    "\n",
    "t0 = time.monotonic()\n",
    "# processing\n",
    "all_histograms, metrics = run(\n",
    "    fileset, \n",
    "    treename, \n",
    "    processor_instance=TtbarAnalysis()\n",
    ")\n",
    "exec_time = time.monotonic() - t0\n",
    "\n",
    "print(f\"\\nexecution took {exec_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feddafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track metrics\n",
    "# utils.metrics.track_metrics(metrics, fileset, exec_time, USE_DASK, USE_SERVICEX, N_FILES_MAX_PER_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb4428",
   "metadata": {},
   "source": [
    "### Inspecting the produced histograms\n",
    "\n",
    "Let's have a look at the data we obtained.\n",
    "We built histograms in two phase space regions, for multiple physics processes and systematic variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd348fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plotting.set_style()\n",
    "\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1, edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\"$\\geq$ 4 jets, 1 b-tag\")\n",
    "plt.xlabel(\"$H_T$ [GeV]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c902d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_histograms[\"hist_dict\"][\"4j2b\"][:, :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\"$\\geq$ 4 jets, $\\geq$ 2 b-tags\")\n",
    "plt.xlabel(\"$m_{bjj}$ [GeV]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3df8b",
   "metadata": {},
   "source": [
    "Our top reconstruction approach ($bjj$ system with largest $p_T$) has worked!\n",
    "\n",
    "Let's also have a look at some systematic variations:\n",
    "- b-tagging, which we implemented as jet-kinematic dependent event weights,\n",
    "- jet energy variations, which vary jet kinematics, resulting in acceptance effects and observable changes.\n",
    "\n",
    "We are making of [UHI](https://uhi.readthedocs.io/) here to re-bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1aabfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b-tagging variations\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), \"ttbar\", \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), \"ttbar\", \"btag_var_0_up\"].plot(label=\"NP 1\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), \"ttbar\", \"btag_var_1_up\"].plot(label=\"NP 2\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), \"ttbar\", \"btag_var_2_up\"].plot(label=\"NP 3\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j1b\"][120j::hist.rebin(2), \"ttbar\", \"btag_var_3_up\"].plot(label=\"NP 4\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$H_T$ [GeV]\")\n",
    "plt.title(\"b-tagging variations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jet energy scale variations\n",
    "all_histograms[\"hist_dict\"][\"4j2b\"][:, \"ttbar\", \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j2b\"][:, \"ttbar\", \"pt_scale_up\"].plot(label=\"scale up\", linewidth=2)\n",
    "all_histograms[\"hist_dict\"][\"4j2b\"][:, \"ttbar\", \"pt_res_up\"].plot(label=\"resolution up\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$m_{bjj}$ [Gev]\")\n",
    "plt.title(\"Jet energy variations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c334dd3",
   "metadata": {},
   "source": [
    "### Save histograms to disk\n",
    "\n",
    "We'll save everything to disk for subsequent usage.\n",
    "This also builds pseudo-data by combining events from the various simulation setups we have processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d05ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.file_output.save_histograms(all_histograms['hist_dict'], \n",
    "                                  fileset, \n",
    "                                  \"histograms.root\", \n",
    "                                  [\"4j1b\", \"4j2b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904cd3c",
   "metadata": {},
   "source": [
    "### Statistical inference\n",
    "\n",
    "A statistical model has been defined in `config.yml`, ready to be used with our output.\n",
    "We will use `cabinetry` to combine all histograms into a `pyhf` workspace and fit the resulting statistical model to the pseudodata we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b89fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cabinetry.configuration.load(\"cabinetry_config.yml\")\n",
    "cabinetry.templates.collect(config)\n",
    "cabinetry.templates.postprocess(config)  # optional post-processing (e.g. smoothing)\n",
    "ws = cabinetry.workspace.build(config)\n",
    "cabinetry.workspace.save(ws, \"workspace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dc601",
   "metadata": {},
   "source": [
    "We can inspect the workspace with `pyhf`, or use `pyhf` to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a83712",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyhf inspect workspace.json | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e4361",
   "metadata": {},
   "source": [
    "Let's try out what we built: the next cell will perform a maximum likelihood fit of our statistical model to the pseudodata we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)\n",
    "\n",
    "cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"ttbar_norm\", close_figure=True, save_figure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd480eec",
   "metadata": {},
   "source": [
    "For this pseudodata, what is the resulting ttbar cross-section divided by the Standard Model prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ffdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_index = model.config.poi_index\n",
    "print(f\"\\nfit result for ttbar_norm: {fit_results.bestfit[poi_index]:.3f} +/- {fit_results.uncertainty[poi_index]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a293479",
   "metadata": {},
   "source": [
    "Let's also visualize the model before and after the fit, in both the regions we are using.\n",
    "The binning here corresponds to the binning used for the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfab0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = cabinetry.model_utils.prediction(model)\n",
    "model_prediction_postfit = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction, data, close_figure=True, config=config)\n",
    "# below method reimplements this visualization in a grid view\n",
    "utils.plotting.plot_data_mc(model_prediction, model_prediction_postfit, data, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce2d14",
   "metadata": {},
   "source": [
    "### What is next?\n",
    "\n",
    "Our next goals for this pipeline demonstration are:\n",
    "- making this analysis even **more feature-complete**,\n",
    "- **addressing performance bottlenecks** revealed by this demonstrator,\n",
    "- **collaborating** with you!\n",
    "\n",
    "Please do not hesitate to get in touch if you would like to join the effort, or are interested in re-implementing (pieces of) the pipeline with different tools!\n",
    "\n",
    "Our mailing list is analysis-grand-challenge@iris-hep.org, sign up via the [Google group](https://groups.google.com/a/iris-hep.org/g/analysis-grand-challenge)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
